# add below layer for Quantization with DeePhi' DECENT
#########################################################################
# CIFAR10 miniVggNet m3
layer {
  name: "data"
  type: "ImageData"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    crop_size: 32
    mean_value: 125
    mean_value: 123
    mean_value: 114
    #mean file is not supported by DeePhi DPU
    #mean_file: "INSERT_ABSOLUTE_PATH_HERE/input/mean.binaryproto"    
  }
  image_data_param {
    source: "INSERT_ABSOLUTE_PATH_HERE/deephi/miniVggNet/quantiz/data/calib/calibration.txt"
    root_folder: "INSERT_ABSOLUTE_PATH_HERE/deephi/miniVggNet/quantiz/data/calib/"  
    batch_size: 10
    shuffle: true
  }
}
# comments below layer for the original TRAIN
###########################################################################
#layer {
#  name: "data"
#  type: "Data"
#  top: "data"
#  top: "label"
#  include {
#   phase: TRAIN
#  }
#  transform_param {
#    mirror: true
#    #crop_size: 32
#    mean_file: "INSERT_ABSOLUTE_PATH_HERE/input/mean.binaryproto"
#  }
#  data_param {
#    source: "INSERT_ABSOLUTE_PATH_HERE/input/lmdb/train_lmdb"
#    batch_size: 128 
#    backend: LMDB
#  }
#}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    mirror: false
    crop_size: 32
    mean_value: 125
    mean_value: 123
    mean_value: 114
  }
  data_param {
    source: "INSERT_ABSOLUTE_PATH_HERE/input/lmdb/valid_lmdb"
    backend: LMDB
    batch_size: 50
  }    
}


######## CONV1=>BN1=>RELU1=>CONV2=>BN2=>RELU2=>POOL1=>DROP1
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  # learning rate and decay multipliers for the filters  
    param {
      lr_mult: 1
      decay_mult: 1
    }
  # learning rate and decay multipliers for the biases    
    param {
      lr_mult: 2
      decay_mult: 0
    }  
  convolution_param {
    num_output: 32
    kernel_size: 3
    stride: 1     
    pad: 1  	  
    weight_filler {    
      #type: "gaussian"
       type: "xavier"      
      #std: 0.0001      
    }
    bias_filler {    
      type: "constant" 
      value: 0
    }    
  }
}
layer {
  name: "bn1"
  type: "BatchNorm"
  bottom: "conv1"
  top: "bn1"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
    #scale_filler {
    #   type: "constant"
    #   value: 1
    #}
    #bias_filler {
    #   type: "constant"
    #   value: 0
    #}
  }
}
layer {
  name: "scale1"
  type: "Scale"
  bottom: "bn1"
  top: "scale1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "scale1"
  top: "relu1"
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "relu1"
  top: "conv2"
  # learning rate and decay multipliers for the filters  
  param {
    lr_mult: 1
    decay_mult: 1
  }
  # learning rate and decay multipliers for the biases    
  param {
    lr_mult: 2
    decay_mult: 0
  }  
  convolution_param {
    num_output: 32     
    kernel_size: 3     
    stride: 1          
    pad: 1  	       
    weight_filler {    
      #type: "gaussian"
       type: "xavier"   
      #std: 0.0001      
    }
    bias_filler {    
      type: "constant" 
      value: 0
    }    
  }
}
layer {
  name: "bn2"
  type: "BatchNorm"
  bottom: "conv2"
  top: "bn2"
  param {
    lr_mult: 0
  } 
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }  
  batch_norm_param {
     use_global_stats: false
    #scale_filler {
    #  type: "constant"
    #  value: 1
    #}
    #bias_filler {
    #  type: "constant"
    #  value: 0
    #}
  }
}
layer {
  name: "scale2"
  type: "Scale"
  bottom: "bn2"
  top: "scale2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "scale2"
  top: "relu2"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "relu2"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "pool1"
  top: "drop1"
  dropout_param {
    dropout_ratio: 0.25
  }
}
######## CONV3=>BN3=>RELU3=>CONV4=>BN4=>RELU4=>POOL2=>DROP2
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "drop1"
  top: "conv3"
  # learning rate and decay multipliers for the filters  
  param {
    lr_mult: 1
    decay_mult: 1
  }
  # learning rate and decay multipliers for the biases    
  param {
    lr_mult: 2
    decay_mult: 0
  }  
  convolution_param {
    num_output: 64     
    kernel_size: 3     
    stride: 1          
    pad: 1  	       
    weight_filler {    
      #type: "gaussian"
      type: "xavier"   
      #std: 0.001      
    }
    bias_filler {    
      type: "constant" 
      value: 0
    }    
  }
}
layer {
  name: "bn3"
  type: "BatchNorm"
  bottom: "conv3"
  top: "bn3"
  param {
    lr_mult: 0
  } 
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
   use_global_stats: false
   #scale_filler {
   #   type: "constant"
   #   value: 1
   # }
   # bias_filler {
   #   type: "constant"
   #   value: 0
   # }
  }
}
layer {
  name: "scale3"
  type: "Scale"
  bottom: "bn3"
  top: "scale3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "scale3"
  top: "relu3"
}

layer {
  name: "conv4"
  type: "Convolution"
  bottom: "relu3"
  top: "conv4"
  # learning rate and decay multipliers for the filters  
    param {
      lr_mult: 1
      decay_mult: 1
    }
  # learning rate and decay multipliers for the biases    
    param { 
      lr_mult: 2
      decay_mult: 0
    }  
  convolution_param {
    num_output: 64     
    kernel_size: 3     
    stride: 1          
    pad: 1  	       
    weight_filler {    
      #type: "gaussian" 
      type: "xavier"  
      #std: 0.001      
    }
    bias_filler {    
      type: "constant" 
      value: 0
    }    
  }
}
layer {
  name: "bn4"
  type: "BatchNorm"
  bottom: "conv4"
  top: "bn4"
   param {
    lr_mult: 0
  } 
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  } 
  batch_norm_param {
    use_global_stats: false
    #scale_filler {
    #  type: "constant"
    #  value: 1
    #}
    #bias_filler {
    #  type: "constant"
    #  value: 0
    #}
  }
}
layer {
  name: "scale4"
  type: "Scale"
  bottom: "bn4"
  top: "scale4"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "scale4"
  top: "relu4"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "relu4"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "drop2"
  type: "Dropout"
  bottom: "pool2"
  top: "drop2"
  dropout_param {
    dropout_ratio: 0.25
  }
}
########  FC1 => RELU5 => BN5 => DROP3
layer {
  name: "fc1"
  type: "InnerProduct"
  bottom: "drop2"
  top: "fc1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 512
    weight_filler {
      #type: "gaussian"
      type: "xavier"  
      #std: 0.001
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "bn5"
  type: "BatchNorm"
  bottom: "fc1"
  top: "bn5"
  param {
    lr_mult: 0
  } 
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  } 
  batch_norm_param {
    use_global_stats: false
    #scale_filler {
    #  type: "constant"
    #  value: 1
    #}
    #bias_filler {
    #  type: "constant"
    #  value: 0
    #}
  }
}
layer {
  name: "scale5"
  type: "Scale"
  bottom: "bn5"
  top: "scale5"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "scale5"
  top: "relu5"
}
layer {
  name: "drop3"
  type: "Dropout"
  bottom: "relu5"
  top: "drop3"
  dropout_param {
     dropout_ratio: 0.5
   }
 }
########  FC2=> SOFTMAX
layer {
  name: "fc2"
  type: "InnerProduct"
  bottom: "drop3"
  top: "fc2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      #type: "gaussian"
      type: "xavier"  
      #std: 0.001
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc2"
  bottom: "label"
  top: "loss"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "accuracy-top1"
  type: "Accuracy"
  bottom: "fc2"
  bottom: "label"
  top: "top-1"
  include {
    phase: TEST
  }
}
layer {
  name: "accuracy-top5"
  type: "Accuracy"
  bottom: "fc2"
  bottom: "label"
  top: "top-5"
  include {
    phase: TEST
  }
  accuracy_param {
    top_k: 5
  }
}
